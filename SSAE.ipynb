{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dropout + Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from codes.utils import *\n",
    "from codes.process import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "random_seed=42\n",
    "bed_root = '/home2/jpark/Projects/prs/data/bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = 'test'\n",
    "fold_num = 1\n",
    "in_fold_num = 1\n",
    "y_value = 'y'\n",
    "ex_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "bed_X_train = pd.read_csv(f\"{get_bed_path('keep_bed_yi_ex', 'train', fold_num, in_fold_num=in_fold_num, y=y_value, ex=ex_num)}.raw\",\n",
    "    delim_whitespace=True)\n",
    "bed_X_train = bed_X_train.iloc[:, [1] + list(range(6, len(bed_X_train.columns)))]\n",
    "bed_y_train = pd.read_csv(get_bed_path('label', 'train', fold_num, in_fold_num=in_fold_num), delim_whitespace=True)[['IID', y_value]]\n",
    "bed_data_train = bed_X_train.merge(bed_y_train, on='IID')\n",
    "\n",
    "# Test data\n",
    "bed_X_test = pd.read_csv(f\"{get_bed_path('keep_bed_yi_ex', 'test', fold_num, in_fold_num=in_fold_num, y=y_value, ex=ex_num)}.raw\",\n",
    "    delim_whitespace=True)\n",
    "bed_X_test = bed_X_test.iloc[:, [1] + list(range(6, len(bed_X_test.columns)))]\n",
    "bed_y_test = pd.read_csv(get_bed_path('label', 'test', fold_num, in_fold_num=in_fold_num), delim_whitespace=True)[['IID', y_value]]\n",
    "bed_data_test = bed_X_test.merge(bed_y_test, on='IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = bed_data_train.drop(['IID', y_value], axis=1).to_numpy().astype(np.float32)\n",
    "y_train = bed_data_train[y_value].to_numpy().astype(np.int32)\n",
    "X_test = bed_data_test.drop(['IID', y_value], axis=1).to_numpy().astype(np.float32)\n",
    "y_test = bed_data_test[y_value].to_numpy().astype(np.int32)\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "n_hidden1 = X_train.shape[1]*2\n",
    "n_outputs = n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "#X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "#X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "#y_train = y_train.astype(np.int32)\n",
    "#y_test = y_test.astype(np.int32)\n",
    "#X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "#y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "def shuffle_X_batch(X, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch = X[batch_idx]\n",
    "        yield X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f426825e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f426825e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f426825e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x7f426825e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f42704e7c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f42704e7c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f42704e7c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f42704e7c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f426825e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f426825e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f426825e9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f426825e9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
    "\n",
    "# For training\n",
    "learning_rate = 0.01\n",
    "\n",
    "# For sparsity\n",
    "sparsity_target = 0.1\n",
    "sparsity_weight = 0.2\n",
    "\n",
    "# For input dropout\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# init\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\n",
    "##### Build Model #####\n",
    "# input\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "\n",
    "# layers\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.sigmoid)  # Input dropout\n",
    "logits = tf.layers.dense(hidden1, n_outputs)\n",
    "\n",
    "# output\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "\n",
    "##### Construct Loss #####\n",
    "# sparsity loss\n",
    "hidden1_mean = tf.reduce_mean(hidden1, axis=0)\n",
    "sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))\n",
    "\n",
    "# cross  entropy \n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)  # Compare with X not X_drop\n",
    "\n",
    "# Loss\n",
    "reconstruction_loss = tf.reduce_mean(xentropy)\n",
    "loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train - Cross entropy:, 0.3461, \tSparsity loss:, 29.1184, \tLoss:, 6.1698\n",
      "0 Test  - Cross entropy:, 0.3520, \tSparsity loss:, 29.6174, \tLoss:, 6.2755\n",
      "1 Train - Cross entropy:, 0.4151, \tSparsity loss:, 8.8636, \tLoss:, 2.1878\n",
      "1 Test  - Cross entropy:, 0.4326, \tSparsity loss:, 9.9727, \tLoss:, 2.4271\n",
      "2 Train - Cross entropy:, 0.4600, \tSparsity loss:, 2.9960, \tLoss:, 1.0592\n",
      "2 Test  - Cross entropy:, 0.4646, \tSparsity loss:, 3.6497, \tLoss:, 1.1946\n",
      "3 Train - Cross entropy:, 0.4546, \tSparsity loss:, 1.3065, \tLoss:, 0.7159\n",
      "3 Test  - Cross entropy:, 0.4532, \tSparsity loss:, 1.4871, \tLoss:, 0.7506\n",
      "4 Train - Cross entropy:, 0.4375, \tSparsity loss:, 0.4013, \tLoss:, 0.5178\n",
      "4 Test  - Cross entropy:, 0.4328, \tSparsity loss:, 0.6522, \tLoss:, 0.5632\n",
      "5 Train - Cross entropy:, 0.4245, \tSparsity loss:, 0.1526, \tLoss:, 0.4551\n",
      "5 Test  - Cross entropy:, 0.4192, \tSparsity loss:, 0.2980, \tLoss:, 0.4788\n",
      "6 Train - Cross entropy:, 0.4229, \tSparsity loss:, 0.0302, \tLoss:, 0.4289\n",
      "6 Test  - Cross entropy:, 0.4116, \tSparsity loss:, 0.1396, \tLoss:, 0.4395\n",
      "7 Train - Cross entropy:, 0.4089, \tSparsity loss:, 0.0303, \tLoss:, 0.4150\n",
      "7 Test  - Cross entropy:, 0.4054, \tSparsity loss:, 0.0676, \tLoss:, 0.4189\n",
      "8 Train - Cross entropy:, 0.3994, \tSparsity loss:, 0.0229, \tLoss:, 0.4040\n",
      "8 Test  - Cross entropy:, 0.3986, \tSparsity loss:, 0.0350, \tLoss:, 0.4056\n",
      "9 Train - Cross entropy:, 0.3910, \tSparsity loss:, 0.0196, \tLoss:, 0.3949\n",
      "9 Test  - Cross entropy:, 0.3912, \tSparsity loss:, 0.0206, \tLoss:, 0.3953\n",
      "10 Train - Cross entropy:, 0.3827, \tSparsity loss:, 0.0146, \tLoss:, 0.3856\n",
      "10 Test  - Cross entropy:, 0.3831, \tSparsity loss:, 0.0146, \tLoss:, 0.3860\n",
      "11 Train - Cross entropy:, 0.3796, \tSparsity loss:, 0.0852, \tLoss:, 0.3967\n",
      "11 Test  - Cross entropy:, 0.3747, \tSparsity loss:, 0.0123, \tLoss:, 0.3771\n",
      "12 Train - Cross entropy:, 0.3632, \tSparsity loss:, 0.0166, \tLoss:, 0.3665\n",
      "12 Test  - Cross entropy:, 0.3660, \tSparsity loss:, 0.0113, \tLoss:, 0.3682\n",
      "13 Train - Cross entropy:, 0.3611, \tSparsity loss:, 0.1435, \tLoss:, 0.3898\n",
      "13 Test  - Cross entropy:, 0.3572, \tSparsity loss:, 0.0105, \tLoss:, 0.3592\n",
      "14 Train - Cross entropy:, 0.3413, \tSparsity loss:, 0.0160, \tLoss:, 0.3445\n",
      "14 Test  - Cross entropy:, 0.3483, \tSparsity loss:, 0.0095, \tLoss:, 0.3502\n",
      "15 Train - Cross entropy:, 0.3324, \tSparsity loss:, 0.0142, \tLoss:, 0.3352\n",
      "15 Test  - Cross entropy:, 0.3396, \tSparsity loss:, 0.0084, \tLoss:, 0.3413\n",
      "16 Train - Cross entropy:, 0.3261, \tSparsity loss:, 0.0213, \tLoss:, 0.3304\n",
      "16 Test  - Cross entropy:, 0.3310, \tSparsity loss:, 0.0071, \tLoss:, 0.3325\n",
      "17 Train - Cross entropy:, 0.3242, \tSparsity loss:, 0.0712, \tLoss:, 0.3385\n",
      "17 Test  - Cross entropy:, 0.3226, \tSparsity loss:, 0.0058, \tLoss:, 0.3237\n",
      "18 Train - Cross entropy:, 0.3109, \tSparsity loss:, 0.0193, \tLoss:, 0.3148\n",
      "18 Test  - Cross entropy:, 0.3143, \tSparsity loss:, 0.0045, \tLoss:, 0.3152\n",
      "19 Train - Cross entropy:, 0.3078, \tSparsity loss:, 0.1138, \tLoss:, 0.3306\n",
      "19 Test  - Cross entropy:, 0.3062, \tSparsity loss:, 0.0035, \tLoss:, 0.3069\n",
      "20 Train - Cross entropy:, 0.2953, \tSparsity loss:, 0.0073, \tLoss:, 0.2968\n",
      "20 Test  - Cross entropy:, 0.2983, \tSparsity loss:, 0.0029, \tLoss:, 0.2989\n",
      "21 Train - Cross entropy:, 0.2843, \tSparsity loss:, 0.0169, \tLoss:, 0.2877\n",
      "21 Test  - Cross entropy:, 0.2906, \tSparsity loss:, 0.0026, \tLoss:, 0.2912\n",
      "22 Train - Cross entropy:, 0.2772, \tSparsity loss:, 0.0043, \tLoss:, 0.2780\n",
      "22 Test  - Cross entropy:, 0.2833, \tSparsity loss:, 0.0027, \tLoss:, 0.2838\n",
      "23 Train - Cross entropy:, 0.2696, \tSparsity loss:, 0.0032, \tLoss:, 0.2702\n",
      "23 Test  - Cross entropy:, 0.2761, \tSparsity loss:, 0.0029, \tLoss:, 0.2767\n",
      "24 Train - Cross entropy:, 0.2666, \tSparsity loss:, 0.0134, \tLoss:, 0.2693\n",
      "24 Test  - Cross entropy:, 0.2691, \tSparsity loss:, 0.0033, \tLoss:, 0.2698\n",
      "25 Train - Cross entropy:, 0.2585, \tSparsity loss:, 0.0145, \tLoss:, 0.2614\n",
      "25 Test  - Cross entropy:, 0.2623, \tSparsity loss:, 0.0040, \tLoss:, 0.2631\n",
      "26 Train - Cross entropy:, 0.2536, \tSparsity loss:, 0.0215, \tLoss:, 0.2579\n",
      "26 Test  - Cross entropy:, 0.2556, \tSparsity loss:, 0.0052, \tLoss:, 0.2566\n",
      "27 Train - Cross entropy:, 0.2495, \tSparsity loss:, 0.0214, \tLoss:, 0.2538\n",
      "27 Test  - Cross entropy:, 0.2490, \tSparsity loss:, 0.0069, \tLoss:, 0.2503\n",
      "28 Train - Cross entropy:, 0.2293, \tSparsity loss:, 0.0127, \tLoss:, 0.2318\n",
      "28 Test  - Cross entropy:, 0.2424, \tSparsity loss:, 0.0093, \tLoss:, 0.2443\n",
      "29 Train - Cross entropy:, 0.2323, \tSparsity loss:, 0.0313, \tLoss:, 0.2386\n",
      "29 Test  - Cross entropy:, 0.2360, \tSparsity loss:, 0.0121, \tLoss:, 0.2384\n",
      "30 Train - Cross entropy:, 0.2221, \tSparsity loss:, 0.0008, \tLoss:, 0.2223\n",
      "30 Test  - Cross entropy:, 0.2297, \tSparsity loss:, 0.0149, \tLoss:, 0.2327\n",
      "31 Train - Cross entropy:, 0.2175, \tSparsity loss:, 0.0095, \tLoss:, 0.2194\n",
      "31 Test  - Cross entropy:, 0.2236, \tSparsity loss:, 0.0178, \tLoss:, 0.2271\n",
      "32 Train - Cross entropy:, 0.2172, \tSparsity loss:, 0.0060, \tLoss:, 0.2184\n",
      "32 Test  - Cross entropy:, 0.2175, \tSparsity loss:, 0.0210, \tLoss:, 0.2217\n",
      "33 Train - Cross entropy:, 0.2039, \tSparsity loss:, 0.0018, \tLoss:, 0.2042\n",
      "33 Test  - Cross entropy:, 0.2116, \tSparsity loss:, 0.0239, \tLoss:, 0.2164\n",
      "34 Train - Cross entropy:, 0.1956, \tSparsity loss:, 0.1156, \tLoss:, 0.2187\n",
      "34 Test  - Cross entropy:, 0.2058, \tSparsity loss:, 0.0264, \tLoss:, 0.2111\n",
      "35 Train - Cross entropy:, 0.1971, \tSparsity loss:, 0.0248, \tLoss:, 0.2021\n",
      "35 Test  - Cross entropy:, 0.2002, \tSparsity loss:, 0.0279, \tLoss:, 0.2058\n",
      "36 Train - Cross entropy:, 0.1827, \tSparsity loss:, 0.0102, \tLoss:, 0.1848\n",
      "36 Test  - Cross entropy:, 0.1946, \tSparsity loss:, 0.0304, \tLoss:, 0.2006\n",
      "37 Train - Cross entropy:, 0.1814, \tSparsity loss:, 0.0045, \tLoss:, 0.1823\n",
      "37 Test  - Cross entropy:, 0.1890, \tSparsity loss:, 0.0333, \tLoss:, 0.1956\n",
      "38 Train - Cross entropy:, 0.1783, \tSparsity loss:, 0.0036, \tLoss:, 0.1790\n",
      "38 Test  - Cross entropy:, 0.1835, \tSparsity loss:, 0.0355, \tLoss:, 0.1906\n",
      "39 Train - Cross entropy:, 0.1693, \tSparsity loss:, 0.0236, \tLoss:, 0.1740\n",
      "39 Test  - Cross entropy:, 0.1781, \tSparsity loss:, 0.0372, \tLoss:, 0.1856\n",
      "40 Train - Cross entropy:, 0.1678, \tSparsity loss:, 0.0090, \tLoss:, 0.1696\n",
      "40 Test  - Cross entropy:, 0.1728, \tSparsity loss:, 0.0386, \tLoss:, 0.1805\n",
      "41 Train - Cross entropy:, 0.1597, \tSparsity loss:, 0.0201, \tLoss:, 0.1637\n",
      "41 Test  - Cross entropy:, 0.1675, \tSparsity loss:, 0.0398, \tLoss:, 0.1755\n",
      "42 Train - Cross entropy:, 0.1553, \tSparsity loss:, 0.0340, \tLoss:, 0.1621\n",
      "42 Test  - Cross entropy:, 0.1623, \tSparsity loss:, 0.0405, \tLoss:, 0.1704\n",
      "43 Train - Cross entropy:, 0.1445, \tSparsity loss:, 0.0974, \tLoss:, 0.1640\n",
      "43 Test  - Cross entropy:, 0.1573, \tSparsity loss:, 0.0400, \tLoss:, 0.1653\n",
      "44 Train - Cross entropy:, 0.1490, \tSparsity loss:, 0.0399, \tLoss:, 0.1570\n",
      "44 Test  - Cross entropy:, 0.1524, \tSparsity loss:, 0.0381, \tLoss:, 0.1601\n",
      "45 Train - Cross entropy:, 0.1398, \tSparsity loss:, 0.0069, \tLoss:, 0.1412\n",
      "45 Test  - Cross entropy:, 0.1476, \tSparsity loss:, 0.0363, \tLoss:, 0.1549\n",
      "46 Train - Cross entropy:, 0.1350, \tSparsity loss:, 0.0027, \tLoss:, 0.1356\n",
      "46 Test  - Cross entropy:, 0.1429, \tSparsity loss:, 0.0343, \tLoss:, 0.1498\n",
      "47 Train - Cross entropy:, 0.1315, \tSparsity loss:, 0.0107, \tLoss:, 0.1336\n",
      "47 Test  - Cross entropy:, 0.1383, \tSparsity loss:, 0.0327, \tLoss:, 0.1448\n",
      "48 Train - Cross entropy:, 0.1249, \tSparsity loss:, 0.0723, \tLoss:, 0.1394\n",
      "48 Test  - Cross entropy:, 0.1336, \tSparsity loss:, 0.0319, \tLoss:, 0.1400\n",
      "49 Train - Cross entropy:, 0.1218, \tSparsity loss:, 0.0042, \tLoss:, 0.1226\n",
      "49 Test  - Cross entropy:, 0.1290, \tSparsity loss:, 0.0308, \tLoss:, 0.1352\n",
      "50 Train - Cross entropy:, 0.1027, \tSparsity loss:, 0.0041, \tLoss:, 0.1035\n",
      "50 Test  - Cross entropy:, 0.1243, \tSparsity loss:, 0.0312, \tLoss:, 0.1306\n",
      "51 Train - Cross entropy:, 0.1095, \tSparsity loss:, 0.0163, \tLoss:, 0.1128\n",
      "51 Test  - Cross entropy:, 0.1197, \tSparsity loss:, 0.0319, \tLoss:, 0.1260\n",
      "52 Train - Cross entropy:, 0.1091, \tSparsity loss:, 0.0064, \tLoss:, 0.1104\n",
      "52 Test  - Cross entropy:, 0.1151, \tSparsity loss:, 0.0324, \tLoss:, 0.1216\n",
      "53 Train - Cross entropy:, 0.1041, \tSparsity loss:, 0.0105, \tLoss:, 0.1062\n",
      "53 Test  - Cross entropy:, 0.1105, \tSparsity loss:, 0.0344, \tLoss:, 0.1174\n",
      "54 Train - Cross entropy:, 0.0992, \tSparsity loss:, 0.0318, \tLoss:, 0.1055\n",
      "54 Test  - Cross entropy:, 0.1060, \tSparsity loss:, 0.0357, \tLoss:, 0.1132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 Train - Cross entropy:, 0.1022, \tSparsity loss:, 0.0189, \tLoss:, 0.1060\n",
      "55 Test  - Cross entropy:, 0.1017, \tSparsity loss:, 0.0358, \tLoss:, 0.1089\n",
      "56 Train - Cross entropy:, 0.0862, \tSparsity loss:, 0.1894, \tLoss:, 0.1241\n",
      "56 Test  - Cross entropy:, 0.0976, \tSparsity loss:, 0.0361, \tLoss:, 0.1048\n",
      "57 Train - Cross entropy:, 0.0841, \tSparsity loss:, 0.0366, \tLoss:, 0.0915\n",
      "57 Test  - Cross entropy:, 0.0936, \tSparsity loss:, 0.0357, \tLoss:, 0.1007\n",
      "58 Train - Cross entropy:, 0.0843, \tSparsity loss:, 0.0136, \tLoss:, 0.0870\n",
      "58 Test  - Cross entropy:, 0.0895, \tSparsity loss:, 0.0354, \tLoss:, 0.0966\n",
      "59 Train - Cross entropy:, 0.0800, \tSparsity loss:, 0.0136, \tLoss:, 0.0827\n",
      "59 Test  - Cross entropy:, 0.0856, \tSparsity loss:, 0.0350, \tLoss:, 0.0926\n",
      "60 Train - Cross entropy:, 0.0771, \tSparsity loss:, 0.0153, \tLoss:, 0.0801\n",
      "60 Test  - Cross entropy:, 0.0816, \tSparsity loss:, 0.0355, \tLoss:, 0.0887\n",
      "61 Train - Cross entropy:, 0.0681, \tSparsity loss:, 0.0057, \tLoss:, 0.0693\n",
      "61 Test  - Cross entropy:, 0.0775, \tSparsity loss:, 0.0369, \tLoss:, 0.0849\n",
      "62 Train - Cross entropy:, 0.0696, \tSparsity loss:, 0.0061, \tLoss:, 0.0708\n",
      "62 Test  - Cross entropy:, 0.0736, \tSparsity loss:, 0.0373, \tLoss:, 0.0811\n",
      "63 Train - Cross entropy:, 0.0657, \tSparsity loss:, 0.0148, \tLoss:, 0.0686\n",
      "63 Test  - Cross entropy:, 0.0696, \tSparsity loss:, 0.0393, \tLoss:, 0.0775\n",
      "64 Train - Cross entropy:, 0.0543, \tSparsity loss:, 0.0366, \tLoss:, 0.0617\n",
      "64 Test  - Cross entropy:, 0.0656, \tSparsity loss:, 0.0423, \tLoss:, 0.0741\n",
      "65 Train - Cross entropy:, 0.0468, \tSparsity loss:, 0.0091, \tLoss:, 0.0486\n",
      "65 Test  - Cross entropy:, 0.0617, \tSparsity loss:, 0.0442, \tLoss:, 0.0705\n",
      "66 Train - Cross entropy:, 0.0451, \tSparsity loss:, 0.0189, \tLoss:, 0.0489\n",
      "66 Test  - Cross entropy:, 0.0578, \tSparsity loss:, 0.0458, \tLoss:, 0.0670\n",
      "67 Train - Cross entropy:, 0.0439, \tSparsity loss:, 0.0112, \tLoss:, 0.0461\n",
      "67 Test  - Cross entropy:, 0.0539, \tSparsity loss:, 0.0480, \tLoss:, 0.0635\n",
      "68 Train - Cross entropy:, 0.0490, \tSparsity loss:, 0.0364, \tLoss:, 0.0563\n",
      "68 Test  - Cross entropy:, 0.0499, \tSparsity loss:, 0.0506, \tLoss:, 0.0601\n",
      "69 Train - Cross entropy:, 0.0447, \tSparsity loss:, 0.0331, \tLoss:, 0.0513\n",
      "69 Test  - Cross entropy:, 0.0459, \tSparsity loss:, 0.0534, \tLoss:, 0.0566\n",
      "70 Train - Cross entropy:, 0.0345, \tSparsity loss:, 0.0951, \tLoss:, 0.0535\n",
      "70 Test  - Cross entropy:, 0.0418, \tSparsity loss:, 0.0574, \tLoss:, 0.0532\n",
      "71 Train - Cross entropy:, 0.0269, \tSparsity loss:, 0.0735, \tLoss:, 0.0416\n",
      "71 Test  - Cross entropy:, 0.0379, \tSparsity loss:, 0.0576, \tLoss:, 0.0494\n",
      "72 Train - Cross entropy:, 0.0263, \tSparsity loss:, 0.0154, \tLoss:, 0.0293\n",
      "72 Test  - Cross entropy:, 0.0340, \tSparsity loss:, 0.0577, \tLoss:, 0.0455\n",
      "73 Train - Cross entropy:, 0.0238, \tSparsity loss:, 0.0159, \tLoss:, 0.0270\n",
      "73 Test  - Cross entropy:, 0.0299, \tSparsity loss:, 0.0594, \tLoss:, 0.0418\n",
      "74 Train - Cross entropy:, 0.0129, \tSparsity loss:, 0.0211, \tLoss:, 0.0172\n",
      "74 Test  - Cross entropy:, 0.0259, \tSparsity loss:, 0.0603, \tLoss:, 0.0380\n",
      "75 Train - Cross entropy:, 0.0149, \tSparsity loss:, 0.0481, \tLoss:, 0.0245\n",
      "75 Test  - Cross entropy:, 0.0221, \tSparsity loss:, 0.0594, \tLoss:, 0.0340\n",
      "76 Train - Cross entropy:, 0.0072, \tSparsity loss:, 0.0409, \tLoss:, 0.0154\n",
      "76 Test  - Cross entropy:, 0.0181, \tSparsity loss:, 0.0596, \tLoss:, 0.0300\n",
      "77 Train - Cross entropy:, 0.0122, \tSparsity loss:, 0.0458, \tLoss:, 0.0213\n",
      "77 Test  - Cross entropy:, 0.0138, \tSparsity loss:, 0.0614, \tLoss:, 0.0261\n",
      "78 Train - Cross entropy:, 0.0035, \tSparsity loss:, 0.0542, \tLoss:, 0.0143\n",
      "78 Test  - Cross entropy:, 0.0094, \tSparsity loss:, 0.0631, \tLoss:, 0.0220\n",
      "79 Train - Cross entropy:, -0.0083, \tSparsity loss:, 0.0254, \tLoss:, -0.0032\n",
      "79 Test  - Cross entropy:, 0.0046, \tSparsity loss:, 0.0667, \tLoss:, 0.0180\n",
      "80 Train - Cross entropy:, -0.0053, \tSparsity loss:, 0.0359, \tLoss:, 0.0018\n",
      "80 Test  - Cross entropy:, -0.0006, \tSparsity loss:, 0.0728, \tLoss:, 0.0139\n",
      "81 Train - Cross entropy:, -0.0127, \tSparsity loss:, 0.1072, \tLoss:, 0.0088\n",
      "81 Test  - Cross entropy:, -0.0068, \tSparsity loss:, 0.0848, \tLoss:, 0.0101\n",
      "82 Train - Cross entropy:, -0.0198, \tSparsity loss:, 0.0615, \tLoss:, -0.0075\n",
      "82 Test  - Cross entropy:, -0.0132, \tSparsity loss:, 0.0948, \tLoss:, 0.0058\n",
      "83 Train - Cross entropy:, -0.0200, \tSparsity loss:, 0.1742, \tLoss:, 0.0148\n",
      "83 Test  - Cross entropy:, -0.0192, \tSparsity loss:, 0.1002, \tLoss:, 0.0009\n",
      "84 Train - Cross entropy:, -0.0278, \tSparsity loss:, 0.0589, \tLoss:, -0.0160\n",
      "84 Test  - Cross entropy:, -0.0253, \tSparsity loss:, 0.1041, \tLoss:, -0.0044\n",
      "85 Train - Cross entropy:, -0.0460, \tSparsity loss:, 0.0754, \tLoss:, -0.0309\n",
      "85 Test  - Cross entropy:, -0.0324, \tSparsity loss:, 0.1123, \tLoss:, -0.0099\n",
      "86 Train - Cross entropy:, -0.0519, \tSparsity loss:, 0.0810, \tLoss:, -0.0357\n",
      "86 Test  - Cross entropy:, -0.0401, \tSparsity loss:, 0.1207, \tLoss:, -0.0160\n",
      "87 Train - Cross entropy:, -0.0379, \tSparsity loss:, 0.1315, \tLoss:, -0.0116\n",
      "87 Test  - Cross entropy:, -0.0483, \tSparsity loss:, 0.1299, \tLoss:, -0.0223\n",
      "88 Train - Cross entropy:, -0.0489, \tSparsity loss:, 0.1037, \tLoss:, -0.0281\n",
      "88 Test  - Cross entropy:, -0.0563, \tSparsity loss:, 0.1370, \tLoss:, -0.0289\n",
      "89 Train - Cross entropy:, -0.0689, \tSparsity loss:, 0.1866, \tLoss:, -0.0316\n",
      "89 Test  - Cross entropy:, -0.0646, \tSparsity loss:, 0.1459, \tLoss:, -0.0354\n",
      "90 Train - Cross entropy:, -0.0996, \tSparsity loss:, 0.1245, \tLoss:, -0.0747\n",
      "90 Test  - Cross entropy:, -0.0733, \tSparsity loss:, 0.1558, \tLoss:, -0.0422\n",
      "91 Train - Cross entropy:, -0.0903, \tSparsity loss:, 0.1503, \tLoss:, -0.0603\n",
      "91 Test  - Cross entropy:, -0.0819, \tSparsity loss:, 0.1645, \tLoss:, -0.0490\n",
      "92 Train - Cross entropy:, -0.0817, \tSparsity loss:, 0.1420, \tLoss:, -0.0533\n",
      "92 Test  - Cross entropy:, -0.0894, \tSparsity loss:, 0.1677, \tLoss:, -0.0558\n",
      "93 Train - Cross entropy:, -0.1090, \tSparsity loss:, 0.1717, \tLoss:, -0.0747\n",
      "93 Test  - Cross entropy:, -0.0982, \tSparsity loss:, 0.1782, \tLoss:, -0.0626\n",
      "94 Train - Cross entropy:, -0.0970, \tSparsity loss:, 0.1601, \tLoss:, -0.0650\n",
      "94 Test  - Cross entropy:, -0.1088, \tSparsity loss:, 0.1975, \tLoss:, -0.0693\n",
      "95 Train - Cross entropy:, -0.1351, \tSparsity loss:, 0.2254, \tLoss:, -0.0900\n",
      "95 Test  - Cross entropy:, -0.1179, \tSparsity loss:, 0.2087, \tLoss:, -0.0761\n",
      "96 Train - Cross entropy:, -0.1396, \tSparsity loss:, 0.1893, \tLoss:, -0.1018\n",
      "96 Test  - Cross entropy:, -0.1249, \tSparsity loss:, 0.2089, \tLoss:, -0.0832\n",
      "97 Train - Cross entropy:, -0.1403, \tSparsity loss:, 0.1657, \tLoss:, -0.1071\n",
      "97 Test  - Cross entropy:, -0.1335, \tSparsity loss:, 0.2158, \tLoss:, -0.0903\n",
      "98 Train - Cross entropy:, -0.1688, \tSparsity loss:, 0.2022, \tLoss:, -0.1284\n",
      "98 Test  - Cross entropy:, -0.1430, \tSparsity loss:, 0.2282, \tLoss:, -0.0974\n",
      "99 Train - Cross entropy:, -0.1536, \tSparsity loss:, 0.2001, \tLoss:, -0.1136\n",
      "99 Test  - Cross entropy:, -0.1517, \tSparsity loss:, 0.2339, \tLoss:, -0.1049\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 1000\n",
    "\n",
    "loss_test_min = 1000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = len(X_train) // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch = next(shuffle_X_batch(X_train, batch_size))\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        r_loss_train, s_loss_train, loss_train = sess.run([reconstruction_loss, sparsity_loss, loss], feed_dict={X: X_batch})\n",
    "        r_loss_test, s_loss_test, loss_test = sess.run([reconstruction_loss, sparsity_loss, loss], feed_dict={X: X_test})\n",
    "        print(f'\\r{epoch} Train - Cross entropy:, {r_loss_train:.4f}, \\tSparsity loss:, {s_loss_train:.4f}, \\tLoss:, {loss_train:.4f}')\n",
    "        print(f'\\r{epoch} Test  - Cross entropy:, {r_loss_test:.4f}, \\tSparsity loss:, {s_loss_test:.4f}, \\tLoss:, {loss_test:.4f}')\n",
    "        if loss_test_min > loss_test:\n",
    "            loss_test_min = loss_test\n",
    "            saver.save(sess, \"./model/my_model_sparse.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /bin2/pkg_python36/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./my_model_sparse.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_reconstructed_digits(X, outputs, model_path = None, n_test_digits = 2):\n",
    "    with tf.Session() as sess:\n",
    "        if model_path:\n",
    "            saver.restore(sess, model_path)\n",
    "#         X_test = mnist.test.images[:n_test_digits]\n",
    "        outputs_val = outputs.eval(feed_dict={X: X_test[:n_test_digits]})\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 3 * n_test_digits))\n",
    "    for digit_index in range(n_test_digits):\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 1)\n",
    "        plot_image(X_test[digit_index])\n",
    "        plt.subplot(n_test_digits, 2, digit_index * 2 + 2)\n",
    "        plot_image(outputs_val[digit_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349da2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349da2e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349da2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349da2e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349d04cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349d04cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349d04cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4349d04cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /bin2/pkg_python36/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid)\n",
    "logits = tf.layers.dense(hidden1, n_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)\n",
    "\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)\n",
    "reconstruction_loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 훈련 MSE: 0.7047666 \t희소 손실: 0.12352318 \t전체 손실: 0.09142406\n",
      "1 훈련 MSE: 0.704796 \t희소 손실: 0.019605529 \t전체 손실: 0.05858742\n",
      "2 훈련 MSE: 0.7048345 \t희소 손실: 0.028799854 \t전체 손실: 0.054506186\n",
      "3 훈련 MSE: 0.70477927 \t희소 손실: 0.008937162 \t전체 손실: 0.045784224\n",
      "4 훈련 MSE: 0.70475614 \t희소 손실: 0.07770297 \t전체 손실: 0.05621028\n",
      "5 훈련 MSE: 0.7046035 \t희소 손실: 0.014777578 \t전체 손실: 0.037430603\n",
      "6 훈련 MSE: 0.7048179 \t희소 손실: 0.010760792 \t전체 손실: 0.032250192\n",
      "7 훈련 MSE: 0.70467716 \t희소 손실: 0.006685371 \t전체 손실: 0.028559342\n",
      "8 훈련 MSE: 0.70488346 \t희소 손실: 0.014640239 \t전체 손실: 0.027989157\n",
      "9 훈련 MSE: 0.7049954 \t희소 손실: 0.010540727 \t전체 손실: 0.025672361\n",
      "10 훈련 MSE: 0.70476043 \t희소 손실: 0.017074438 \t전체 손실: 0.025290703\n",
      "11 훈련 MSE: 0.7047554 \t희소 손실: 0.03934033 \t전체 손실: 0.029316306\n",
      "12 훈련 MSE: 0.70469606 \t희소 손실: 0.019014362 \t전체 손실: 0.024640704\n",
      "13 훈련 MSE: 0.70488685 \t희소 손실: 0.030627202 \t전체 손실: 0.025302013\n",
      "14 훈련 MSE: 0.70481926 \t희소 손실: 0.014720345 \t전체 손실: 0.020566199\n",
      "15 훈련 MSE: 0.7045547 \t희소 손실: 0.03210661 \t전체 손실: 0.023378175\n",
      "16 훈련 MSE: 0.7047551 \t희소 손실: 0.016374063 \t전체 손실: 0.020019844\n",
      "17 훈련 MSE: 0.7048573 \t희소 손실: 0.014826399 \t전체 손실: 0.019578878\n",
      "18 훈련 MSE: 0.70477533 \t희소 손실: 0.023401786 \t전체 손실: 0.020934856\n",
      "19 훈련 MSE: 0.70472986 \t희소 손실: 0.021583013 \t전체 손실: 0.019544411\n",
      "20 훈련 MSE: 0.7048758 \t희소 손실: 0.063320175 \t전체 손실: 0.027692698\n",
      "21 훈련 MSE: 0.70483166 \t희소 손실: 0.051129993 \t전체 손실: 0.02441512\n",
      "22 훈련 MSE: 0.7048301 \t희소 손실: 0.057694986 \t전체 손실: 0.026278095\n",
      "23 훈련 MSE: 0.70484006 \t희소 손실: 0.019251088 \t전체 손실: 0.018211883\n",
      "24 훈련 MSE: 0.7046763 \t희소 손실: 0.017222723 \t전체 손실: 0.017566694\n",
      "25 훈련 MSE: 0.7047307 \t희소 손실: 0.019990468 \t전체 손실: 0.01739691\n",
      "26 훈련 MSE: 0.7049238 \t희소 손실: 0.04430051 \t전체 손실: 0.023023173\n",
      "27 훈련 MSE: 0.7047136 \t희소 손실: 0.046981275 \t전체 손실: 0.023599796\n",
      "28 훈련 MSE: 0.7045709 \t희소 손실: 0.039977424 \t전체 손실: 0.02142844\n",
      "29 훈련 MSE: 0.7049996 \t희소 손실: 0.06922811 \t전체 손실: 0.028003555\n",
      "30 훈련 MSE: 0.70485514 \t희소 손실: 0.022128401 \t전체 손실: 0.017567098\n",
      "31 훈련 MSE: 0.70476186 \t희소 손실: 0.062447727 \t전체 손실: 0.025119763\n",
      "32 훈련 MSE: 0.7045395 \t희소 손실: 0.038811553 \t전체 손실: 0.0205707\n",
      "33 훈련 MSE: 0.7047245 \t희소 손실: 0.04363279 \t전체 손실: 0.021302916\n",
      "34 훈련 MSE: 0.70466316 \t희소 손실: 0.043620918 \t전체 손실: 0.020632096\n",
      "35 훈련 MSE: 0.7046484 \t희소 손실: 0.026722973 \t전체 손실: 0.017461685\n",
      "36 훈련 MSE: 0.7045358 \t희소 손실: 0.027665464 \t전체 손실: 0.017213417\n",
      "37 훈련 MSE: 0.70476717 \t희소 손실: 0.029253626 \t전체 손실: 0.01754222\n",
      "38 훈련 MSE: 0.70467514 \t희소 손실: 0.05645452 \t전체 손실: 0.023179524\n",
      "39 훈련 MSE: 0.70476866 \t희소 손실: 0.037683845 \t전체 손실: 0.019191563\n",
      "40 훈련 MSE: 0.7048187 \t희소 손실: 0.03307556 \t전체 손실: 0.017912507\n",
      "41 훈련 MSE: 0.7047399 \t희소 손실: 0.035527527 \t전체 손실: 0.018570889\n",
      "42 훈련 MSE: 0.70485914 \t희소 손실: 0.029943487 \t전체 손실: 0.017299853\n",
      "43 훈련 MSE: 0.70495915 \t희소 손실: 0.054906715 \t전체 손실: 0.022754617\n",
      "44 훈련 MSE: 0.7049049 \t희소 손실: 0.032150615 \t전체 손실: 0.01809168\n",
      "45 훈련 MSE: 0.7049343 \t희소 손실: 0.054782584 \t전체 손실: 0.022084907\n",
      "46 훈련 MSE: 0.7046562 \t희소 손실: 0.036546916 \t전체 손실: 0.01830202\n",
      "47 훈련 MSE: 0.7048627 \t희소 손실: 0.04285598 \t전체 손실: 0.01913426\n",
      "48 훈련 MSE: 0.70483756 \t희소 손실: 0.04221758 \t전체 손실: 0.019337103\n",
      "49 훈련 MSE: 0.70456356 \t희소 손실: 0.050503954 \t전체 손실: 0.02192307\n",
      "50 훈련 MSE: 0.704849 \t희소 손실: 0.032584067 \t전체 손실: 0.01826416\n",
      "51 훈련 MSE: 0.7050262 \t희소 손실: 0.050802454 \t전체 손실: 0.021666948\n",
      "52 훈련 MSE: 0.70474577 \t희소 손실: 0.027270457 \t전체 손실: 0.016154775\n",
      "53 훈련 MSE: 0.70461047 \t희소 손실: 0.049078796 \t전체 손실: 0.021041099\n",
      "54 훈련 MSE: 0.7049404 \t희소 손실: 0.06027465 \t전체 손실: 0.023400078\n",
      "55 훈련 MSE: 0.704887 \t희소 손실: 0.13866566 \t전체 손실: 0.038282342\n",
      "56 훈련 MSE: 0.7049574 \t희소 손실: 0.040830113 \t전체 손실: 0.019480802\n",
      "57 훈련 MSE: 0.7048474 \t희소 손실: 0.06804258 \t전체 손실: 0.024142468\n",
      "58 훈련 MSE: 0.70490736 \t희소 손실: 0.07371303 \t전체 손실: 0.025353152\n",
      "59 훈련 MSE: 0.7048388 \t희소 손실: 0.05028642 \t전체 손실: 0.020932727\n",
      "60 훈련 MSE: 0.70479083 \t희소 손실: 0.03540106 \t전체 손실: 0.017771833\n",
      "61 훈련 MSE: 0.70493966 \t희소 손실: 0.043558907 \t전체 손실: 0.018896509\n",
      "62 훈련 MSE: 0.70486355 \t희소 손실: 0.042882703 \t전체 손실: 0.018792689\n",
      "63 훈련 MSE: 0.70471096 \t희소 손실: 0.04156083 \t전체 손실: 0.01881256\n",
      "64 훈련 MSE: 0.7045731 \t희소 손실: 0.04649141 \t전체 손실: 0.019597847\n",
      "65 훈련 MSE: 0.7047675 \t희소 손실: 0.05490572 \t전체 손실: 0.021400344\n",
      "66 훈련 MSE: 0.704478 \t희소 손실: 0.057606034 \t전체 손실: 0.021899268\n",
      "67 훈련 MSE: 0.70473117 \t희소 손실: 0.06165259 \t전체 손실: 0.022753023\n",
      "68 훈련 MSE: 0.7047347 \t희소 손실: 0.0539909 \t전체 손실: 0.021262221\n",
      "69 훈련 MSE: 0.70458245 \t희소 손실: 0.03995867 \t전체 손실: 0.018706633\n",
      "70 훈련 MSE: 0.70483875 \t희소 손실: 0.05473187 \t전체 손실: 0.021372091\n",
      "71 훈련 MSE: 0.70470655 \t희소 손실: 0.049876384 \t전체 손실: 0.020722505\n",
      "72 훈련 MSE: 0.70475495 \t희소 손실: 0.064231575 \t전체 손실: 0.023715252\n",
      "73 훈련 MSE: 0.7048407 \t희소 손실: 0.06482697 \t전체 손실: 0.023236074\n",
      "74 훈련 MSE: 0.7046737 \t희소 손실: 0.07983266 \t전체 손실: 0.025930448\n",
      "75 훈련 MSE: 0.70480746 \t희소 손실: 0.04487704 \t전체 손실: 0.019332971\n",
      "76 훈련 MSE: 0.70502543 \t희소 손실: 0.10607215 \t전체 손실: 0.030738227\n",
      "77 훈련 MSE: 0.70471036 \t희소 손실: 0.07365614 \t전체 손실: 0.024953093\n",
      "78 훈련 MSE: 0.7045609 \t희소 손실: 0.054648962 \t전체 손실: 0.020791333\n",
      "79 훈련 MSE: 0.7048357 \t희소 손실: 0.06136309 \t전체 손실: 0.022674348\n",
      "80 훈련 MSE: 0.7048542 \t희소 손실: 0.06847518 \t전체 손실: 0.02390036\n",
      "81 훈련 MSE: 0.7047953 \t희소 손실: 0.07997737 \t전체 손실: 0.025382046\n",
      "82 훈련 MSE: 0.70481974 \t희소 손실: 0.070328325 \t전체 손실: 0.024620019\n",
      "83 훈련 MSE: 0.7047722 \t희소 손실: 0.044937093 \t전체 손실: 0.019087782\n",
      "84 훈련 MSE: 0.7048807 \t희소 손실: 0.05482406 \t전체 손실: 0.020829547\n",
      "85 훈련 MSE: 0.70477843 \t희소 손실: 0.072529726 \t전체 손실: 0.024967756\n",
      "86 훈련 MSE: 0.7049862 \t희소 손실: 0.074650966 \t전체 손실: 0.025132779\n",
      "87 훈련 MSE: 0.7048109 \t희소 손실: 0.048156057 \t전체 손실: 0.020079378\n",
      "88 훈련 MSE: 0.7045681 \t희소 손실: 0.042674508 \t전체 손실: 0.019218262\n",
      "89 훈련 MSE: 0.7049342 \t희소 손실: 0.091343276 \t전체 손실: 0.02935951\n",
      "90 훈련 MSE: 0.70475096 \t희소 손실: 0.10890689 \t전체 손실: 0.0320644\n",
      "91 훈련 MSE: 0.7048048 \t희소 손실: 0.14743108 \t전체 손실: 0.039894674\n",
      "92 훈련 MSE: 0.7048771 \t희소 손실: 0.13553852 \t전체 손실: 0.038833402\n",
      "93 훈련 MSE: 0.70489264 \t희소 손실: 0.056466527 \t전체 손실: 0.021886986\n",
      "94 훈련 MSE: 0.7045744 \t희소 손실: 0.040551834 \t전체 손실: 0.018315908\n",
      "95 훈련 MSE: 0.7047106 \t희소 손실: 0.041130543 \t전체 손실: 0.01899023\n",
      "96 훈련 MSE: 0.70486784 \t희소 손실: 0.036343668 \t전체 손실: 0.017288603\n",
      "97 훈련 MSE: 0.70480984 \t희소 손실: 0.10057676 \t전체 손실: 0.030357333\n",
      "98 훈련 MSE: 0.70474297 \t희소 손실: 0.059611574 \t전체 손실: 0.022280049\n",
      "99 훈련 MSE: 0.7050048 \t희소 손실: 0.06945995 \t전체 손실: 0.024482772\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = len(X_train) // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = next(shuffle_batch(X_train, y_train, batch_size))\n",
    "            sess.run(training_op, feed_dict={X: X_batch})\n",
    "        reconstruction_loss_val, sparsity_loss_val, loss_val = sess.run([reconstruction_loss, sparsity_loss, loss], feed_dict={X: X_batch})\n",
    "        print(\"\\r{}\".format(epoch), \"훈련 MSE:\", reconstruction_loss_val, \"\\t희소 손실:\", sparsity_loss_val, \"\\t전체 손실:\", loss_val)\n",
    "        saver.save(sess, \"./my_model_sparse.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
